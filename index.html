<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="一瓶子不满，半瓶子晃悠。">
<meta property="og:type" content="website">
<meta property="og:title" content="ZYYZ">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="ZYYZ">
<meta property="og:description" content="一瓶子不满，半瓶子晃悠。">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ZYYZ">
<meta name="twitter:description" content="一瓶子不满，半瓶子晃悠。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>ZYYZ</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZYYZ</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">hello</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/13/Word2Vec原理（二）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/13/Word2Vec原理（二）/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-13T15:17:30+08:00">
                2018-11-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="noopener">转载自word2vec原理(二) 基于Hierarchical Softmax的模型</a><br>在word2vec原理(一)CBOW与Skip-Gram模型基础中，我们讲到了使用神经网络的方法来得到词向量语言模型的原理和一些问题，现在我们开始关注word2vec的语言模型如何改进传统的神经网络的方法。由于word2vec有两种改进方法，一种是基于Hierarchical Softmax的，另一种是基于Negative Sampling的。本文关注于基于Hierarchical Softmax的改进方法，在下一篇讨论基于Negative Sampling的改进方法。  </p>
<h4 id="1-基于Hierarchical-Softmax的模型概述"><a href="#1-基于Hierarchical-Softmax的模型概述" class="headerlink" title="1 基于Hierarchical Softmax的模型概述"></a>1 基于Hierarchical Softmax的模型概述</h4><p>我们先回顾下传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。这个模型如下图所示。其中V是词汇表的大小。<br><img src="\picture\1042406-20170727105326843-18935623.png" alt="image"><br>word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)。由于这里是从多个词向量变成了一个词向量。  </p>
<p>第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。我们在上一节已经介绍了霍夫曼树的原理。如何映射呢？这里就是理解word2vec的关键所在了。  </p>
<p>由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一颗二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词w2。<br><img src="\picture\1042406-20170727105752968-819608237.png" alt="image"><br>和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为”Hierarchical Softmax”。  </p>
<p>如何“沿着霍夫曼树一步步完成”呢？在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即：  </p>
<script type="math/tex; mode=display">P(+)=\sigma{(x^T_w\theta)}=\frac{1}{1+e^{-x^T_w\theta}}</script><p>其中$x_w$是当前内部节点的词向量，而$\theta$则是我们需要从训练样本求出的逻辑回归的模型参数。<br>使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为V,现在变成了$log_2V$。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。</p>
<p>容易理解，被划分为左子树而成为负类的概率为P(−)=1−P(+)。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看P(−),P(+)谁的概率值大。而控制P(−),P(+)谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数$\theta$。</p>
<p>对于上图中的$w_2$，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点n($w_2$,1)的P(−)概率大，n($w_2$,2)的P(−)概率大，n($w_2$,3)的P(+)概率大。</p>
<p>回到基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点$\theta$,使训练样本达到最大似然。那么如何达到最大似然呢？</p>
<h4 id="2-基于Hierarchical-Softmax的模型梯度计算"><a href="#2-基于Hierarchical-Softmax的模型梯度计算" class="headerlink" title="2 基于Hierarchical Softmax的模型梯度计算"></a>2 基于Hierarchical Softmax的模型梯度计算</h4><p>我们使用最大似然法来寻找所有节点的词向量和所有内部节点$\theta$。先拿上面的$w_2$例子来看，我们期望最大化下面的似然函数：  </p>
<script type="math/tex; mode=display">\prod_{i=1}^3P(n(w_i),i)=(1-\frac{1}{1+e^{-x^T_w\theta_1}})(1-\frac{1}{1+e^{-x^T_w\theta_2}})\frac{1}{1+e^{-x^T_w\theta_3}}</script><p>对于所有的训练样本，我们期望最大化所有样本的似然函数乘积。<br>为了便于我们后面一般化的描述，我们定义输入的词为$w$,其从输入层词向量求和平均后的霍夫曼树根节点词向量为$x_w$, 从根节点到$w$所在的叶子节点，包含的节点总数为$l_w$, $w$在霍夫曼树中从根节点开始，经过的第$i$个节点表示为$p_i^w$,对应的霍夫曼编码为$d_i^w\in(0,1)$,其中$i=2,3,…l_w$。而该节点对应的模型参数表示为$\theta_i^w$, 其中$i=1,2,…l_w-1$，没有$i=l_w$是因为模型参数仅仅针对于霍夫曼树的内部节点。<br>定义$w$经过的霍夫曼树某一个节点j的逻辑回归概率为$P(d_j^w|x_w,\theta_{j-1}^w)$，其表达式为：  </p>
<script type="math/tex; mode=display">P(d_j^w|x_w,\theta_{j-1}^w)=\begin{cases} \sigma{(x^T_w\theta_{j-1})},& d_j^w=0 \\ 1-\sigma{(x^T_w\theta_{j-1})},& d_j^w=1 \end{cases}</script><p>那么对于某一个目标输出词$w$,其最大似然为：  </p>
<script type="math/tex; mode=display">\prod_{j=2}^{l_w}P(d_j^w|x_w,\theta_{j-1}^w)=\prod_{j=2}^{l_w}[\sigma{(x^T_w\theta_{j-1})}]^{1-d_j^w}[1-\sigma{(x^T_w\theta_{j-1})}]^{d_j^w}</script><p>在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量。这样我们可以得到$w$的对数似然函数$L$如下：  </p>
<script type="math/tex; mode=display">L=\log\prod_{j=2}^{l_w}P(d_j^w|x_w,\theta_{j-1}^w)=\sum_{j=2}^{l_w}((1-d_j^w)\log[\sigma{(x^T_w\theta_{j-1})}]+d_j^w\log[1-\sigma{(x^T_w\theta_{j-1})}])</script><p>要得到模型中w词向量和内部节点的模型参数$\theta$, 我们使用梯度上升法即可。首先我们求模型参数$\theta_{j-1}^w$的梯度：  </p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial \theta_{j-1}^w}=(1-d_j^w)\frac{\sigma{(x^T_w\theta_{j-1}^w)}(1-\sigma{(x^T_w\theta_{j-1}^w)})}{\sigma{(x^T_w\theta_{j-1}^w)}}x_w-d_j^w\frac{\sigma{(x^T_w\theta_{j-1}^w)}(1-\sigma{(x^T_w\theta_{j-1}^w)})}{1-\sigma{(x^T_w\theta_{j-1}^w)}}x_w</script><script type="math/tex; mode=display">=(1-d_j^w)(1-\sigma{(x^T_w\theta_{j-1}^w)})x_w-d_j^w\sigma{(x^T_w\theta_{j-1}^w)}x_w</script><script type="math/tex; mode=display">=(1-d_j^w-\sigma{(x^T_w\theta_{j-1}^w)})x_w</script><p>如果大家看过之前写的逻辑回归原理小结，会发现这里的梯度推导过程基本类似。</p>
<p>同样的方法，可以求出$x_w$的梯度表达式如下：</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial x_w}=\sum_{j=2}^{l_w}(1-d_j^w-\sigma{(x^T_w\theta_{j-1}^w)})\theta_{j-1}^w</script><p>有了梯度表达式，我们就可以用梯度上升法进行迭代来一步步的求解我们需要的所有的$\theta_{j-1}^w$和$x_w$。</p>
<h4 id="3-基于Hierarchical-Softmax的CBOW模型"><a href="#3-基于Hierarchical-Softmax的CBOW模型" class="headerlink" title="3 基于Hierarchical Softmax的CBOW模型"></a>3 基于Hierarchical Softmax的CBOW模型</h4><p>由于word2vec有两种模型：CBOW和Skip-Gram,我们先看看基于CBOW模型时， Hierarchical Softmax如何使用。</p>
<p>首先我们要定义词向量的维度大小M，以及CBOW的上下文大小2c,这样我们对于训练样本中的每一个词，其前面的c个词和后面的c个词作为了CBOW模型的输入,该词本身作为样本的输出，期望softmax概率最大。</p>
<p>在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。</p>
<p>对于从输入层到隐藏层（投影层），这一步比较简单，就是对w周围的2c个词向量求和取平均即可，即：</p>
<script type="math/tex; mode=display">x_w=\frac{1}{2c}\sum_{i=1}^{2c}x_i</script><p>第二步，通过梯度上升法来更新我们的$\theta_{j-1}^w$和$x_w$，注意这里的$x_w$是由2c个词向量相加而成，我们做梯度更新完毕后会用梯度项直接更新原始的各个$x_i(i=1,2,,,,2c)$，即：</p>
<script type="math/tex; mode=display">\theta_{j-1}^w=\theta_{j-1}^w+\eta(1-d_j^w-\sigma{(x^T_w\theta_{j-1}^w)})x_w</script><script type="math/tex; mode=display">x_w =x_w +\eta\sum_{j=2}^{l_w}(1-d_j^w-\sigma{(x^T_w\theta_{j-1}^w)})\theta_{j-1}^w,(i=1,2,...,2c)</script><p>其中$\eta$为梯度上升法的步长。<br>这里总结下基于Hierarchical Softmax的CBOW模型算法流程，梯度迭代使用了随机梯度上升法：</p>
<p>输入：基于CBOW的语料训练样本，词向量的维度大小M，CBOW的上下文大小2c,步长$\eta$<br>输出：霍夫曼树的内部节点模型参数$\theta$，所有的词向量w</p>
<ol>
<li>基于语料训练样本建立霍夫曼树。</li>
<li>随机初始化所有的模型参数$\theta$，所有的词向量w</li>
<li>进行梯度上升迭代过程，对于训练集中的每一个样本(context(w),w)做如下处理：<br>a) e=0,计算$x_w=\frac{1}{2c}\sum_{i=1}^{2c}x_i$。<br>b) for j = 2 to $l_w$, 计算：  <script type="math/tex; mode=display">f=\sigma(x^T_w\theta_{j-1}^w)</script></li>
</ol>
<script type="math/tex; mode=display">g=(1-d_j^w-f)\eta</script><script type="math/tex; mode=display">e=e+g\theta_{j-1}^w</script><script type="math/tex; mode=display">\theta_{j-1}^w=\theta_{j-1}^w+gx_w</script><p>c) 对于context(w)中的每一个词向量$x_i$(共2c个)进行更新： </p>
<script type="math/tex; mode=display">x_i=x_i+e</script><p>d) 如果梯度收敛，则结束梯度迭代，否则回到步骤3继续迭代。</p>
<h4 id="4-基于Hierarchical-Softmax的Skip-Gram模型"><a href="#4-基于Hierarchical-Softmax的Skip-Gram模型" class="headerlink" title="4 基于Hierarchical Softmax的Skip-Gram模型"></a>4 基于Hierarchical Softmax的Skip-Gram模型</h4><p>现在我们先看看基于Skip-Gram模型时， Hierarchical Softmax如何使用。此时输入的只有一个词w,输出的为2c个词向量context(w)。</p>
<p>我们对于训练样本中的每一个词，该词本身作为样本的输入， 其前面的c个词和后面的c个词作为了Skip-Gram模型的输出,，期望这些词的softmax概率比其他的词大。</p>
<p>Skip-Gram模型和CBOW模型其实是反过来的，在上一篇已经讲过。</p>
<p>在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。</p>
<p>对于从输入层到隐藏层（投影层），这一步比CBOW简单，由于只有一个词，所以，即$x_w$就是词w对应的词向量。</p>
<p>第二步，通过梯度上升法来更新我们的$\theta_{j-1}^w$和$x_w$，注意这里的$x_w$周围有2c个词向量，此时如果我们期望$p(x_i|x_w),i=1,2…2c$最大。此时我们注意到由于上下文是相互的，在期望$p(x_i|x_w),i=1,2…2c$最大化的同时，反过来我们也期$p(x_w|x_i),i=1,2…2c$最大。那么是使用$p(x_i|x_w)$好还是P(xw|xi)好呢，word2vec使用了后者，这样做的好处就是在一个迭代窗口内，我们不是只更新$x_w$一个词，而是$x_i,-=1,2…2c$共2c个词。这样整体的迭代会更加的均衡。因为这个原因，Skip-Gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对2c个输出进行迭代更新。</p>
<p>这里总结下基于Hierarchical Softmax的Skip-Gram模型算法流程，梯度迭代使用了随机梯度上升法：</p>
<p>输入：基于Skip-Gram的语料训练样本，词向量的维度大小M，Skip-Gram的上下文大小2c,步长$\eta$<br>输出：霍夫曼树的内部节点模型参数$\theta$，所有的词向量w。  </p>
<ol>
<li>基于语料训练样本建立霍夫曼树。  </li>
<li>随机初始化所有的模型参数$\theta$，所有的词向量w,</li>
<li>进行梯度上升迭代过程，对于训练集中的每一个样本(w,context(w))做如下处理：<br>a)  for i =1 to 2c:<br>i) e=0<br>ii) for j = 2 to $l_w$, 计算：  </li>
</ol>
<script type="math/tex; mode=display">f=\sigma(x^T_w\theta_{j-1}^w)</script><script type="math/tex; mode=display">g=(1-d_j^w-f)\eta</script><script type="math/tex; mode=display">e=e+g\theta_{j-1}^w</script><script type="math/tex; mode=display">\theta_{j-1}^w=\theta_{j-1}^w+gx_w</script><p>iii)$x_i=x_i+e$<br>b)如果梯度收敛，则结束梯度迭代，算法结束，否则回到步骤a继续迭代。  </p>
<h4 id="5-Hierarchical-Softmax的模型源码和算法的对应"><a href="#5-Hierarchical-Softmax的模型源码和算法的对应" class="headerlink" title="5 Hierarchical Softmax的模型源码和算法的对应　"></a>5 Hierarchical Softmax的模型源码和算法的对应　</h4><p>这里给出上面算法和<a href="https://github.com/tmikolov/word2vec/blob/master/word2vec.c" target="_blank" rel="noopener">word2vec源码</a>中的变量对应关系。</p>
<p>在源代码中，基于Hierarchical Softmax的CBOW模型算法在435-463行，基于Hierarchical Softmax的Skip-Gram的模型算法在495-519行。大家可以对着源代码再深入研究下算法。</p>
<p>在源代码中，neule对应我们上面的e, syn0对应我们的$x_w$, syn1对应我们的$\theta_{j-1}^i$, layer1_size对应词向量的维度，window对应我们的c。</p>
<p>另外，vocab[word].code[d]指的是，当前单词word的，第d个编码，编码不含Root结点。vocab[word].point[d]指的是，当前单词word，第d个编码下，前置的结点。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/13/Word2Vec原理（一）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/13/Word2Vec原理（一）/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-13T15:08:58+08:00">
                2018-11-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">转载自word2vec原理(一)CBOW与Skip-Gram模型基础</a><br>word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。虽然源码是开源的，但是谷歌的代码库国内无法访问，因此本文的讲解word2vec原理以Github上的word2vec代码为准。本文关注于word2vec的基础知识。  </p>
<h4 id="1-词向量基础"><a href="#1-词向量基础" class="headerlink" title="1 词向量基础"></a>1 词向量基础</h4><p>用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词”Queen”的序号为2， 那么它的词向量就是(0,1,0,0,0)。同样的道理，词”Woman”的词向量就是(0,0,0,1,0)。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation.<br><img src="\picture\1042406-20170713145606275-2100371803.png" alt="image"><br>One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？</p>
<p>Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。</p>
<p>比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。<br><img src="\picture\1042406-20170713150625759-1047275185.png" alt="image"><br>有了用Dristributed representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现：  </p>
<script type="math/tex; mode=display">\overrightarrow{King}-\overrightarrow{Man}+\overrightarrow{Woman}=\overrightarrow{Queen}</script><p><img src="\picture\1042406-20170713151608181-1336632086.png" alt="image"><br>可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。</p>
<h4 id="2-CBOW与Skip-Gram用于神经网络语言模型"><a href="#2-CBOW与Skip-Gram用于神经网络语言模型" class="headerlink" title="2 CBOW与Skip-Gram用于神经网络语言模型"></a>2 CBOW与Skip-Gram用于神经网络语言模型</h4><p>在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。</p>
<p>这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。</p>
<p>CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。  </p>
<p>这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。<br><img src="\picture\1042406-20170713152436931-1817493891.png" alt="image"><br>Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为4， 特定的这个词”Learning”是我们的输入，而这8个上下文词是我们的输出。</p>
<p>这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前8的8个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某1个词对应的最可能的8个上下文词时，我们可以通过一次DNN前向传播算法得到概率大小排前8的softmax概率对应的神经元所对应的词即可。</p>
<p>以上就是神经网络语言模型中如何用CBOW与Skip-Gram来训练模型与得到词向量的大概过程。但是这和word2vec中用CBOW与Skip-Gram来训练模型与得到词向量的过程有很多的不同。</p>
<p>word2vec为什么不用现成的DNN模型，要继续优化出新方法呢？最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，这意味着我们DNN的输出层需要进行softmax计算各个词的输出概率的的计算量很大。有没有简化一点点的方法呢？  </p>
<h4 id="3-word2vec基础之霍夫曼树"><a href="#3-word2vec基础之霍夫曼树" class="headerlink" title="3 word2vec基础之霍夫曼树"></a>3 word2vec基础之霍夫曼树</h4><p>word2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。</p>
<p>具体如何用霍夫曼树来进行CBOW和Skip-Gram的训练我们在下一节讲，这里我们先复习下霍夫曼树。</p>
<p><strong>霍夫曼树的建立其实并不难，过程如下：</strong></p>
<p>输入：权值为(w1,w2,…wn)的n个节点</p>
<p>输出：对应的霍夫曼树</p>
<ol>
<li><p>将(w1,w2,…wn)看做是有n棵树的森林，每个树仅有一个节点。</p>
</li>
<li><p>在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和。</p>
</li>
<li>将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林。</li>
<li>重复步骤2和3直到森林里只有一棵树为止。  </li>
</ol>
<p>下面我们用一个具体的例子来说明霍夫曼树建立的过程，我们有(a,b,c,d,e,f)共6个节点，节点的权值分布是(16,4,8,6,20,3)。</p>
<p>首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是16,8,6,20,7。此时根节点权重最小的6,7合并，得到新子树，依次类推，最终得到下面的霍夫曼树。<br><img src="\picture\1042406-20170713161800009-962272008.png" alt="image"><br>那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1.如上图，则可以得到c的编码是00。<br>在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/13/从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/13/从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史/" itemprop="url">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-13T14:58:00+08:00">
                2018-11-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/自然语言处理/" itemprop="url" rel="index">
                    <span itemprop="name">自然语言处理</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">【转载自-从Word   Embedding到Bert模型—自然语言处理中的预训练技术发展史】</a><br><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">Google的Bert GitHub代码</a></p>
<p>Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/11/13/从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/29/RNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/29/RNN/" itemprop="url">循环神经网络（Recurrent Neural Network）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-29T16:55:00+08:00">
                2018-10-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>Q：<br>RNN的循环次数用什么来衡量？是否是step数？<br>每次循环就相当于一个是时间序列，每个循环输入的input的x是否是所有的x？<br>RNN类似于MLP原始的感知机，只是每个时间序列中的隐藏层的参数总是共享的。  </p>
<h4 id="1-神经网络-Neural-Network"><a href="#1-神经网络-Neural-Network" class="headerlink" title="1 神经网络(Neural Network)"></a>1 神经网络(Neural Network)</h4><p>神经网络的结构如下入图所示，想要了解RNN网络NN是基础，RNN也是保持着这个结构，有输入层、隐藏层和输出层。<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/10/29/RNN/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/24/中秋节/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/24/中秋节/" itemprop="url">中秋节</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-24T16:30:29+08:00">
                2018-09-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/私人记录/" itemprop="url" rel="index">
                    <span itemprop="name">私人记录</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h4 id="【关于感情】"><a href="#【关于感情】" class="headerlink" title="【关于感情】"></a>【关于感情】</h4><ul>
<li>最近被朋友调侃说我现在正处在青春期，具体症状就是化妆、起痘痘、买买买还有谈恋爱。我觉得呢。。。说的有点道理，我也不知道当年年轻的时候我都在干嘛了，一副什么都无所谓的表情，对男生和化妆打扮也不感兴趣，好像对什么都没什么太大兴趣。我觉得呢这个兴趣不是我主动去选择对什么感兴趣的，而是被动吸引，某个人，某件事真的吸引到我从而引起了我的兴趣。
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/09/24/中秋节/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/24/hexo显示公式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/24/hexo显示公式/" itemprop="url">hexo显示公式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-24T15:05:10+08:00">
                2018-09-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/工具/" itemprop="url" rel="index">
                    <span itemprop="name">工具</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h3 id="本篇博客的生成是为了解决上一篇博客的问题"><a href="#本篇博客的生成是为了解决上一篇博客的问题" class="headerlink" title="本篇博客的生成是为了解决上一篇博客的问题"></a>本篇博客的生成是为了解决上一篇博客的问题</h3><ul>
<li>markdown怎么编辑公式<br><a href="https://www.zybuluo.com/codeep/note/163962" target="_blank" rel="noopener">具体参见这个链接</a><br>应该也没啥说的了，这个链接很全了
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/09/24/hexo显示公式/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/23/逻辑回归（Logistic Regression）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/23/逻辑回归（Logistic Regression）/" itemprop="url">逻辑回归（Logistic Regression）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-23T16:48:00+08:00">
                2018-09-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/基础算法/" itemprop="url" rel="index">
                    <span itemprop="name">基础算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><h4 id="逻辑回归VS线性回归"><a href="#逻辑回归VS线性回归" class="headerlink" title="逻辑回归VS线性回归"></a>逻辑回归VS线性回归</h4><p>逻辑回归不是回归（线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法），而是二分类器，分类结果<script type="math/tex">y</script>对影响因素<script type="math/tex">x_i</script>之间的一种多变量分析方法，判断多个变量对于结果的影响。<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/09/23/逻辑回归（Logistic Regression）/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/18/卷积神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/18/卷积神经网络/" itemprop="url">卷积神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-18T15:12:08+08:00">
                2018-05-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="大概介绍一下"><a href="#大概介绍一下" class="headerlink" title="大概介绍一下"></a>大概介绍一下</h1><h3 id="【为什么要用到卷积神经网络？】"><a href="#【为什么要用到卷积神经网络？】" class="headerlink" title="【为什么要用到卷积神经网络？】"></a>【为什么要用到卷积神经网络？】</h3><p>答：减少每层网络权重参数，加快计算速度，一张图片都是很大的例如一张1000*1000的图片输入到一个全连接的神经网络，输入是3000，则需要的权重数为1000*1000*3000个，这个数字是很大的。卷积神经网络的卷积层的应用能减少每层需要的参数，每个卷积层设置了一个filter过滤器，例如一个3*3的filter，如果该层需要描述10个特征（每个过滤器描述一个特征，10个特征就是设置10个过滤器），所以有（3*3+1）*10=90个参数（加1是偏置），而且<strong>无论输入数据量多大，该层需要的参数都不变，只和设置的过滤器的大小和描述的特征数有关。</strong><br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/05/18/卷积神经网络/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/18/hello/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/18/hello/" itemprop="url">程序员惯例————hello word！！！</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-18T14:44:44+08:00">
                2018-05-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="用博客记录技术、生活，从这里开始。"><a href="#用博客记录技术、生活，从这里开始。" class="headerlink" title="用博客记录技术、生活，从这里开始。  "></a>用博客记录技术、生活，从这里开始。  </h2><p>愿所有的等待都是值得的~</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/18/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZYYZ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/picture/image.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZYYZ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/18/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-18T14:21:55+08:00">
                2018-05-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/05/18/hello-world/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/picture/image.png"
                alt="ZYYZ" />
            
              <p class="site-author-name" itemprop="name">ZYYZ</p>
              <p class="site-description motion-element" itemprop="description">一瓶子不满，半瓶子晃悠。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhoujingyi110" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:363663584@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZYYZ</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
